{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDeAPSBJUiMz",
        "outputId": "b16420b7-4e62-47b6-b1df-ab310a656d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "fatal: destination path 'gpt2-fa-poetry' already exists and is not an empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: ز بالا چو پی بر زمین برنهاد\n",
            "Tokenized Input: [286, 846, 13052, 625, 327, 762, 327, 15156]\n",
            "Decoded Input: ز بالا چو پی بر زمین برنهاد\n",
            "Number of samples in the training set: 34725\n",
            "Number of samples in the validation set: 14882\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(42001, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/HooshvareLab/gpt2-fa-poetry\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/ferdousi.txt'\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "lines = lines[2:]\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "def initialize_tokenizer_and_config(model_name_or_path, output_directory):\n",
        "    tokenizer = create_tokenizer(model_name_or_path, output_directory)\n",
        "    config = create_config(model_name_or_path, tokenizer, output_directory)\n",
        "    return tokenizer, config\n",
        "\n",
        "def create_tokenizer(model_name_or_path, output_directory):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        bos_token='<s>',\n",
        "        eos_token='</s>',\n",
        "        pad_token='<pad>',\n",
        "        unk_token='<unk>'\n",
        "    )\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"bos_token\": '</s>',\n",
        "        \"eos_token\": '</s>',\n",
        "        \"pad_token\": '<pad>',\n",
        "        \"unk_token\": '<unk>'\n",
        "    })\n",
        "    tokenizer.save_pretrained(output_directory)\n",
        "    return tokenizer\n",
        "\n",
        "def create_config(model_name_or_path, tokenizer, output_directory):\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0],\n",
        "        eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0],\n",
        "        pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n",
        "        unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n",
        "    )\n",
        "    config.save_pretrained(output_directory)\n",
        "    return config\n",
        "\n",
        "model_name_or_path = \"HooshvareLab/gpt2-fa\"\n",
        "output_directory = \"./gpt2-fa/\"\n",
        "\n",
        "tokenizer, config = initialize_tokenizer_and_config(model_name_or_path, output_directory)\n",
        "df_input, df_target, df_concat = [], [], []\n",
        "\n",
        "for i in range(0, len(lines) - 3, 2):\n",
        "    df_input.append(lines[i])\n",
        "    df_target.append(' <s> ' + lines[i + 1] + '   ' + lines[i + 2] + '    ' + lines[i + 3] + '  </s>  ')\n",
        "    df_concat.append(lines[i] + ' <s> ' + lines[i + 1] + '   ' + lines[i + 2] + '    ' + lines[i + 3] + '  </s>  ')\n",
        "\n",
        "# Sample input text\n",
        "input_text = df_input[1000]\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoding = tokenizer.encode(input_text)\n",
        "\n",
        "# Decode the tokenized input back to text\n",
        "decoded_input = tokenizer.decode(input_encoding)\n",
        "\n",
        "# Print results\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"Tokenized Input:\", input_encoding)\n",
        "print(\"Decoded Input:\", decoded_input)\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_texts, tokenizer, max_length=1024):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        for txt in input_texts:\n",
        "            encodings_dict = tokenizer(\n",
        "                txt, truncation=True, max_length=max_length, padding=\"max_length\"\n",
        "            )\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "# Define the maximum sequence length\n",
        "max_seq = 256\n",
        "\n",
        "# Create an instance of the CustomDataset\n",
        "dataset = CustomDataset(df_concat, tokenizer, max_length=max_seq)\n",
        "\n",
        "# Define the percentage of data to use for testing (e.g., 20%)\n",
        "test_percentage = 0.3\n",
        "total_samples = len(dataset)\n",
        "test_size = int(total_samples * test_percentage)\n",
        "train_size = total_samples - test_size\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"Number of samples in the training set: {len(train_dataset)}\")\n",
        "print(f\"Number of samples in the validation set: {len(test_dataset)}\")\n",
        "\n",
        "# Loading the model configuration and setting it to the GPT2 standard settings.\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 2\n",
        "warmup_steps = 1e2\n",
        "sample_every = 300\n",
        "from transformers import AdamW\n",
        "\n",
        "import torch\n",
        "# Define RMSProp optimizer\n",
        "optimizer = torch.optim.RMSprop(\n",
        "    model.parameters(),\n",
        "    lr=5e-4,\n",
        "    alpha=0.99,\n",
        "    eps=1e-8,\n",
        "    centered=False,\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=RandomSampler(train_dataset),\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    sampler=SequentialSampler(test_dataset),\n",
        "    batch_size=8\n",
        ")\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "CEvtOje1eRv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sample_input = df_input[np.random.randint(0, len(df_input))]\n",
        "print(sample_input)\n",
        "sample_input_ids = torch.tensor(tokenizer([sample_input])[\"input_ids\"])\n",
        "sample_input_ids = sample_input_ids.to('cpu')\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "    input_ids=sample_input_ids,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    max_length=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=5\n",
        ")\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    output = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
        "    output = output.replace(\"<|startoftext|>\", \"\\n\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<sep>\", \"\\n\")\n",
        "\n",
        "    print(f'output: {output}')"
      ],
      "metadata": {
        "id": "wwvtbmf8fa3T",
        "outputId": "64665146-5bc3-489b-e07f-d1f10e125897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "برآنگونه گرد اندر آمد سپاه\n",
            "output: برآنگونه گرد اندر آمد سپاه وی و سربازان وی در پشت سپاه گرد اندر رفت که این کار به شکست انجامید و سرانجام سپاه به فرماندهی پیروس سردار رومی پس از پنج روز جنگ از رم به شهر رسید و سپاهیان روم به پیروزی دست یافتند.\n",
            "output: برآنگونه گرد اندر آمد سپاه چین را از آن سو فرستاد، چین را با آنان رو‌به‌رو ساخت و بدین‌سان در پی وی تاخت و تازهای چین‌رو را به‌دنبال آورد. با نزدیک شدن چین، چین\n",
            "output: برآنگونه گرد اندر آمد سپاه را برای این که به سوی رود، بر ایشان گشوده شود، بر گرد اندر آمد، از سوی غرب، که میان آن‌ها می‌رسد، به سوی مغرب، بر گرد اندر نشست (نزدیک اندرشد\n",
            "output: برآنگونه گرد اندر آمد سپاه و جنگ است و به جنگ در میان نیست. و چون جنگ رخ نداد، با شمشیر استمداد استمداد و نیزهٔ پربرگردانان، بر او زد و او را گرفت. در سال ۴۶۹ میلادی\n",
            "output: برآنگونه گرد اندر آمد سپاه در حال فزونی به سوی پایتخت بود. چون فرمانده‌ای در حال طغیان بود، بر او دست درازی می‌شد تا از این رهگذر به جنگ بپردازد. در این میان، سالار او نیز با سپاهیان روبرو گشت\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer\n",
        "import torch\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    # Add distributed training options\n",
        "    per_device_eval_batch_size=32,\n",
        "    fp16=False,  # Enable mixed-precision training if supported\n",
        "    dataloader_num_workers=16,  # Adjust based on your system capabilities\n",
        "    report_to=\"tensorboard\",  # You can use TensorBoard for logging\n",
        ")\n",
        "\n",
        "# Initialize Trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "total_t0 = time.time()\n",
        "training_stats = []\n",
        "# Check if a GPU is available, and if so, use it; otherwise, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "for epoch_i in tqdm(range(training_args.num_train_epochs),position=0):\n",
        "    print(f'Beginning epoch {epoch_i + 1} of {training_args.num_train_epochs}')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks, token_type_ids=None)\n",
        "        loss = outputs.loss\n",
        "        loss = loss.mean()\n",
        "        batch_loss = loss\n",
        "        total_train_loss += batch_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print()\n",
        "    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n",
        "    print()\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader, total=len(validation_dataloader), position=0):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            loss = loss.mean()\n",
        "            batch_loss = loss\n",
        "        total_eval_loss += batch_loss\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    print()\n",
        "    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n",
        "    print()\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(f'Total training took {format_time(time.time()-total_t0)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "2GWiyUYzgC7L",
        "outputId": "a140c048-efb3-4610-ab97-10218b5052a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning epoch 1 of 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4341/4341 [37:27<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Training Loss: 0.5336850881576538. Epoch time: 0:37:28\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1861/1861 [05:23<00:00,  5.76it/s]\n",
            "100%|██████████| 1/1 [42:50<00:00, 2570.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation loss: 0.39344531297683716. Validation Time: 0:05:23\n",
            "\n",
            "Total training took 0:42:51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define the number of times you want to run the code\n",
        "num_iterations = 10  # You can change this to the desired number of iterations\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    # Change the name of sample_input to input_data\n",
        "    input_data = df_input[np.random.randint(0, len(df_input))]\n",
        "    print(input_data)\n",
        "\n",
        "    # Change sample_input_ids to input_ids\n",
        "    input_ids = torch.tensor(tokenizer([input_data])[\"input_ids\"])\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    # Change sample_outputs to generated_sequences\n",
        "    generated_sequences = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        max_length=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Update variable names in the loop as well\n",
        "    for i, generated_sequence in enumerate(generated_sequences): out = tokenizer.decode(generated_sequence, skip_special_tokens=False).replace(\"\\n\", \"\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<sep>\", \"\\n\"); print(f'output: {out}')\n",
        "\n"
      ],
      "metadata": {
        "id": "S3YKQkVX6ER4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7742038a-28a8-4ec2-e670-a1863cee6cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خود و سرکشان سوی جیحون کشید\n",
            "output: خود و سرکشان سوی جیحون کشید  به خون از در تیره اندیشه اندر کشید   سوی بیشهٔ اژدها شد دراز    جهانی برو پر ز آتش همی کرد باز  \n",
            "چنین داد پاسخ که چرخ بلند\n",
            "output: چنین داد پاسخ که چرخ بلند  بلند آسمان را دهد بر بلند   همان بد که در بزم جوید همی    سپهرش روانها به زهر آب بفگنیم  \n",
            "ببردند چیزی که شایسته بود\n",
            "output: ببردند چیزی که شایسته بود  به یزدان جز از باد هرگز مباد   ز گفتار او شاد شد شهریار    بشد شاد و خندان به ایوان نگار  \n",
            "سه یک زان نخستین بدرویش داد\n",
            "output: سه یک زان نخستین بدرویش داد  ازان پس به فرجامش نوید   به مهر اندرون نامه بود    ز گیتی یکی بهره بود  \n",
            "گر ایدونک با من تو پیمان کنی\n",
            "output: گر ایدونک با من تو پیمان کنی  به فرمانت گروگان کنی   گر ای دون که او را کنی جای خویش    بدانگه که آید به پیمان خویش  \n",
            "سپه بودش از جنگیان صدهزار\n",
            "output: سپه بودش از جنگیان صدهزار  ببردند گردان آن کارزار   بفرمود تا پیش او برنشست    سوی رزمگه لشکر آراستند  \n",
            "به شاه جهان گفت کای شهریار\n",
            "output: به شاه جهان گفت کای شهریار  توی پرگنهکار از روزگار   ترا دل به کین از پس اندر دمان    مکن تیره خورشید روشن روان  \n",
            "ز آهرمنان بفگند شست گرد\n",
            "output: ز آهرمنان بفگند شست گرد  به خاک اندرون پیل و مردی نبرد   ز گیتی یکی لشکر آمد فراز    یکی باد سرد از جگر بر فراز  \n",
            "بفرمود و گفت ار بماند یکی\n",
            "output: بفرمود و گفت ار بماند یکی  ببایدت رفتن مرا اندکی   کنون هرچ بایستشان پند ما    بباید که گردد به پیوند ما  \n",
            "نبیرهٔ جهانجوی کاوس کی\n",
            "output: نبیرهٔ جهانجوی کاوس کی  که با فر و فرهنگ و رای و پی   جهاندار پیروز بنواختشان    بریشان همی خواندند آفرین  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!python -m nltk.downloader popular\n",
        "\n",
        "# Example usage\n",
        "reference = \"به نام خداوند جان و خرد   \tکزین برتر اندیشه برنگذرد     خداوند نام و خداوند جای     خداوند روزی ده رهنمای  \"\n",
        "candidate = \"بفرمود و گفت ار بماند یکی  ببایدت رفتن مرا اندکی   کنون هرچ بایستشان پند ما    بباید که گردد به پیوند ما  \"\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def calculate_bleu(reference_texts, candidate_text):\n",
        "    reference_tokens = [word_tokenize(ref.lower()) for ref in reference_texts]\n",
        "    candidate_tokens = word_tokenize(candidate_text.lower())\n",
        "    score = sentence_bleu(reference_tokens, candidate_tokens)\n",
        "    return score\n",
        "\n",
        "bleu_score = calculate_bleu(reference, candidate)\n",
        "print(\"**************************************************************\")\n",
        "print(\"**************************************************************\")\n",
        "print(\"**************************************************************\")\n",
        "print(\"**************************************************************\")\n",
        "print(\"**************************************************************\")\n",
        "print(\"**************************************************************\")\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDlwLQgbSq2P",
        "outputId": "d240b166-7339-426c-ffee-bfcf82c8faf1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "**************************************************************\n",
            "**************************************************************\n",
            "**************************************************************\n",
            "**************************************************************\n",
            "**************************************************************\n",
            "**************************************************************\n",
            "BLEU Score: 8.510469113101058e-232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b2LOoknteU4",
        "outputId": "2e26f714-039e-4b79-f935-7c1748c233ff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "\n",
        "def tokenize(text):\n",
        "    # Split text into tokens (words)\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "def get_word_frequency(tokens):\n",
        "    # Count the frequency of each word in tokens\n",
        "    word_frequency = {}\n",
        "    for token in tokens:\n",
        "        if token in word_frequency:\n",
        "            word_frequency[token] += 1\n",
        "        else:\n",
        "            word_frequency[token] = 1\n",
        "    return word_frequency\n",
        "\n",
        "def cosine_similarity(reference, candidate):\n",
        "    # Tokenize the reference and candidate strings\n",
        "    ref_tokens = tokenize(reference)\n",
        "    cand_tokens = tokenize(candidate)\n",
        "\n",
        "    # Get word frequency vectors for reference and candidate\n",
        "    ref_word_freq = get_word_frequency(ref_tokens)\n",
        "    cand_word_freq = get_word_frequency(cand_tokens)\n",
        "\n",
        "    # Calculate dot product and magnitudes\n",
        "    dot_product = sum(ref_word_freq.get(word, 0) * cand_word_freq.get(word, 0) for word in set(ref_tokens) & set(cand_tokens))\n",
        "    ref_magnitude = math.sqrt(sum(ref_word_freq[word] ** 2 for word in ref_tokens))\n",
        "    cand_magnitude = math.sqrt(sum(cand_word_freq[word] ** 2 for word in cand_tokens))\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    if ref_magnitude == 0 or cand_magnitude == 0:\n",
        "        return 0  # Handle division by zero\n",
        "    else:\n",
        "        return dot_product / (ref_magnitude * cand_magnitude)\n",
        "\n",
        "# Given reference and candidate strings\n",
        "reference = \"به نام خداوند جان و خرد   \tکزین برتر اندیشه برنگذرد     خداوند نام و خداوند جای     خداوند روزی ده رهنمای  \"\n",
        "candidate = \"بفرمود و گفت ار بماند یکی  ببایدت رفتن مرا اندکی   کنون هرچ بایستشان پند ما    بباید که گردد به پیوند ما  \"\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity = cosine_similarity(reference, candidate)\n",
        "print(f\"Cosine Similarity: {similarity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_G_iUSAt5Sp",
        "outputId": "439e7ada-f615-4f42-affe-674fadd76619"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.06\n"
          ]
        }
      ]
    }
  ]
}